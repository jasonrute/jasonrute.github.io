# Portfolio

Here are some cool things I have done.

## Classifying call center calls

<img src="canstockphoto2706055.jpg" alt="Solving Rubik’s cube via deep reinforcement learning" height="300"/>

In contract work for an outbound call center, I used supervised learning to automatically classify phone call transcripts into a half dozen industry categories ("voicemail", "not interested", "wrong number", etc.)  I achieved an accuracy of 84%, exceeding the companies expectations, which will free up agents from having to manually label calls.

Specifically, I started with transcripts of the phone calls generated by a third party service.  I used a "bag-of-words" approach where I converted each transcript into a collection of pairs of adjacent words weighted by their uniqueness to the message.  Then I trained a classifier via multiclass logistic regression with regularization.  This resulted in a simple, interpretable, fast, and robust classifier which can quickly classify the phone call transcripts.

## Solving Rubik’s cube via deep reinforcement learning

<a href="https://github.com/jasonrute/puzzle_cube"><img src="https://foter.com/photos/395/rubik-cube-cube-game-puzzle-rubik-toy-square.jpg" alt="Solving Rubik’s cube via deep reinforcement learning" height="300"/></a>

I used a variation of Deepmind's AlphaZero algorithm for Go (and Chess) to learn to solve a Rubik's cube with using no prior knowledge.

Specifically, I use a deep 3D convolutional neural network to compute a policy (next best move) and valuation (shortest number of moves to solution).  This network is trained via reinforcment learning, where the algorithm uses Monte Carlo tree search to find for a solution to a randomly shuffled cube.  These solutions are used, in turn, to train the next generation of the network.  This new network is used to guide the next round of Monte Carlo tree search, and so on.

See [here](https://github.com/jasonrute/puzzle_cube) for more.

## Training a neural network to play a racing game

<a href="https://github.com/jasonrute/csb_neural_network"><img src="bot_nn_genetic.gif" alt="Training a neural network to play a racing game" height="300"/></a>

In [Coders Strike Back](https://www.codingame.com/multiplayer/bot-programming/coders-strike-back) one programs a bot to race through an open course, touching each checkpoint in order.  Using reinforcement learning I have trained a bot to be able to quickly race through the course even after being knocked around by the other bots.

Specifically, I use a small feed-forward neural network to compute a policy (next best move).  This network is trained via reinforcment learning, where an optimization algorithm uses Monte Carlo search (specifically a form of evolutionary algorithm) to search for the most optimal path in randomly generated race courses.  These solutions are, in turn, used to train the next generation of network.  This new network is used to seed the next round of optimization, and so on.

See [here](https://github.com/jasonrute/csb_neural_network) for more.